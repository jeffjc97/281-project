{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from collections import OrderedDict\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, BayesianRidge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from torch.autograd import Variable\n",
    "from sklearn.feature_selection import mutual_info_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hdrs = None\n",
    "data = None\n",
    "\n",
    "# parse data from song features\n",
    "with open(\"data/random_song_features.csv\", \"r\") as f:\n",
    "    reader = csv.reader(f)\n",
    "    all_rows = list(reader)\n",
    "    hdrs = all_rows[0]\n",
    "    data = all_rows[1:]\n",
    "    \n",
    "data = np.array(data)\n",
    "np.random.shuffle(data)\n",
    "pop_index = hdrs.index('popularity')\n",
    "all_id = data[:,:2]\n",
    "data = data[:,2:]\n",
    "y_data = data[:,pop_index - 2]\n",
    "x_data = np.delete(data, pop_index - 2, 1)\n",
    "y_data = y_data.astype(np.float)\n",
    "x_data = x_data.astype(np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 39060\n",
      "Test:  4340\n"
     ]
    }
   ],
   "source": [
    "N_ROWS = len(x_data)\n",
    "N_TRAIN = int(0.9 * N_ROWS)\n",
    "N_TEST = N_ROWS - N_TRAIN\n",
    "x_train = x_data[:N_TRAIN]\n",
    "y_train = y_data[:N_TRAIN]\n",
    "x_test = x_data[N_TRAIN:]\n",
    "y_test = y_data[N_TRAIN:]\n",
    "print(\"Train: {}\\nTest:  {}\".format(N_TRAIN, N_ROWS - N_TRAIN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting on regressor linear\tFit time: 0.03\n",
      "Starting on regressor bayesian\tFit time: 0.05\n",
      "Starting on regressor mlp\tFit time: 2.02\n",
      "Starting on regressor linSVR\tFit time: 9.24\n",
      "Starting on regressor elastic\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinzhang/anaconda/lib/python3.6/site-packages/sklearn/linear_model/coordinate_descent.py:484: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Fitting data with very small alpha may cause precision problems.\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 1.67\n",
      "Starting on regressor lasso\tFit time: 1.65\n",
      "Starting on regressor forest-1\tFit time: 0.84\n",
      "Starting on regressor forest-2\tFit time: 0.93\n",
      "Starting on regressor forest-3\tFit time: 0.93\n",
      "Starting on regressor forest-4\tFit time: 0.93\n",
      "Starting on regressor forest-5\tFit time: 1.04\n",
      "Starting on regressor forest-10\tFit time: 2.16\n",
      "Starting on regressor forest-20\tFit time: 3.32\n",
      "Starting on regressor forest-30\tFit time: 4.74\n",
      "Starting on regressor forest-40\tFit time: 6.69\n",
      "Starting on regressor forest-50\tFit time: 8.04\n",
      "Starting on regressor forest-100\tFit time: 15.24\n",
      "Starting on regressor bagger-1\tFit time: 0.80\n",
      "Starting on regressor bagger-2\tFit time: 1.15\n",
      "Starting on regressor bagger-3\tFit time: 1.15\n",
      "Starting on regressor bagger-4\tFit time: 1.25\n",
      "Starting on regressor bagger-5\tFit time: 1.37\n",
      "Starting on regressor bagger-10\tFit time: 2.50\n",
      "Starting on regressor bagger-20\tFit time: 4.09\n",
      "Starting on regressor bagger-30\tFit time: 6.05\n",
      "Starting on regressor bagger-40\tFit time: 8.02\n",
      "Starting on regressor bagger-50\tFit time: 11.62\n",
      "Starting on regressor bagger-100\tFit time: 19.53\n",
      "Starting on regressor extrat-1\tFit time: 0.32\n",
      "Starting on regressor extrat-2\tFit time: 0.32\n",
      "Starting on regressor extrat-3\tFit time: 0.32\n",
      "Starting on regressor extrat-4\tFit time: 0.32\n",
      "Starting on regressor extrat-5\tFit time: 0.42\n",
      "Starting on regressor extrat-10\tFit time: 0.73\n",
      "Starting on regressor extrat-20\tFit time: 1.15\n",
      "Starting on regressor extrat-30\tFit time: 1.76\n",
      "Starting on regressor extrat-40\tFit time: 2.27\n",
      "Starting on regressor extrat-50\tFit time: 2.90\n",
      "Starting on regressor extrat-100\tFit time: 5.48\n",
      "Starting on regressor gboost-1\tFit time: 0.18\n",
      "Starting on regressor gboost-2\tFit time: 0.28\n",
      "Starting on regressor gboost-3\tFit time: 0.38\n",
      "Starting on regressor gboost-4\tFit time: 0.45\n",
      "Starting on regressor gboost-5\tFit time: 0.54\n",
      "Starting on regressor gboost-10\tFit time: 0.96\n",
      "Starting on regressor gboost-20\tFit time: 1.78\n",
      "Starting on regressor aboost-1\tFit time: 1.18\n",
      "Starting on regressor aboost-2\tFit time: 2.14\n",
      "Starting on regressor aboost-3\tFit time: 3.15\n",
      "Starting on regressor aboost-4\tFit time: 4.26\n",
      "Starting on regressor aboost-5\tFit time: 5.30\n",
      "Starting on regressor aboost-10\tFit time: 10.28\n",
      "Starting on regressor aboost-20\tFit time: 19.77\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import \\\n",
    "    LinearRegression, LogisticRegression, BayesianRidge, ElasticNet, Lasso\n",
    "from sklearn.ensemble import \\\n",
    "    RandomForestRegressor, AdaBoostRegressor, BaggingRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.makedirs('results')\n",
    "\n",
    "regressors = {}\n",
    "random_regressors = {\n",
    "    'linear':   LinearRegression,\n",
    "    'bayesian': BayesianRidge,\n",
    "    'mlp':      MLPRegressor,\n",
    "    'linSVR':   LinearSVR,\n",
    "    'elastic':  ElasticNet,\n",
    "    'lasso':    Lasso\n",
    "}\n",
    "fast_regressors = {\n",
    "    'forest': ('Random Forest', RandomForestRegressor),\n",
    "    'bagger': ('Bagging', BaggingRegressor),\n",
    "    'extrat': ('Extra Trees', ExtraTreesRegressor)\n",
    "}\n",
    "slow_regressors = {\n",
    "    'gboost': ('Gradient Boosting', GradientBoostingRegressor),\n",
    "    'aboost': ('AdaBoost Boosting', AdaBoostRegressor)\n",
    "}\n",
    "fast_estimators = [1, 2, 3, 4, 5, 10, 20, 30, 40, 50, 100]\n",
    "slow_estimators = [1, 2, 3, 4, 5, 10, 20]\n",
    "\n",
    "for key in random_regressors:\n",
    "    regressors[key] = random_regressors[key]()\n",
    "for key in fast_regressors:\n",
    "    for i in fast_estimators:\n",
    "        regressor = fast_regressors[key][1]\n",
    "        regressors[key + '-{}'.format(i)] = regressor(n_jobs=8, n_estimators=i)\n",
    "for key in slow_regressors:\n",
    "    for i in slow_estimators:\n",
    "        regressor = slow_regressors[key][1]\n",
    "        regressors[key + '-{}'.format(i)] = regressor(n_estimators=i)\n",
    "\n",
    "data = {}\n",
    "for name, regressor in regressors.items():\n",
    "    print(\"Starting on regressor {}     \".format(name), end='\\t')\n",
    "    start_time = time.time()\n",
    "    regressor.fit(x_train, y_train)\n",
    "    end_time = time.time()\n",
    "    print(\"Fit time: {:.2f}\".format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done analyzing results!\n"
     ]
    }
   ],
   "source": [
    "for name, regressor in regressors.items():\n",
    "    print(\"Analyzing results for regressor {}...  \".format(name), end='\\t')\n",
    "    pred = regressor.predict(x_test)\n",
    "    errs = Variable(torch.Tensor(pred - y_test))\n",
    "    loss = errs.pow(2).sum()\n",
    "    avg_err = errs.abs().mean().data.numpy()[0]\n",
    "    rmse = np.sqrt(loss.data.numpy()[0] / N_TEST)\n",
    "    data[name] = [avg_err, rmse]\n",
    "    \n",
    "    plt.clf()\n",
    "    plt.hist(errs.data.numpy(), bins=100)\n",
    "    plt.xlabel(\"Popularity Prediction Error\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.title(\n",
    "        'Prediction Errors for Model \\\"{}\\\"'\n",
    "        '\\nAverage Error: {:4.2f}, Test RMSE: {:4.2f}'.format(\n",
    "        name, avg_err, rmse\n",
    "    ))\n",
    "    plt.savefig(\"results/v2-split90-{}.png\".format(name))\n",
    "    print(\"Average Error: {:4.2f}, Test RMSE: {:4.2f}\".format(\n",
    "        avg_err, rmse\n",
    "    ))\n",
    "\n",
    "with open(\"results/v2-split90-data.csv\", \"w+\") as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Model', 'Average Error', 'RMSE'])\n",
    "    for model in data:\n",
    "        writer.writerow([model] + data[model])\n",
    "\n",
    "plt.clf()\n",
    "for key in fast_regressors:\n",
    "    plt.plot(fast_estimators, list(map(\n",
    "        lambda e: data[key + '-{}'.format(e)][0], fast_estimators\n",
    "    )), label=fast_regressors[key][0] + \"-L1\")\n",
    "for key in fast_regressors:\n",
    "    plt.plot(fast_estimators, list(map(\n",
    "        lambda e: data[key + '-{}'.format(e)][1], fast_estimators\n",
    "    )), label=fast_regressors[key][0] + \"-L2\")\n",
    "plt.xlabel('# Estimators')\n",
    "plt.ylabel('Norm')\n",
    "plt.title('L1/L2 Norms vs. # Estimators for Parallelizable Regressors')\n",
    "plt.legend()\n",
    "plt.savefig('results/norm_fast.png', bbox_inches='tight', pad_inches=0.2)\n",
    "\n",
    "plt.clf()\n",
    "for key in slow_regressors:\n",
    "    plt.plot(slow_estimators, list(map(\n",
    "        lambda e: data[key + '-{}'.format(e)][0], slow_estimators\n",
    "    )), label=slow_regressors[key][0] + \"-L1\")\n",
    "for key in slow_regressors:\n",
    "    plt.plot(slow_estimators, list(map(\n",
    "        lambda e: data[key + '-{}'.format(e)][1], slow_estimators\n",
    "    )), label=slow_regressors[key][0] + \"-L2\")\n",
    "plt.xlabel('# Estimators')\n",
    "plt.ylabel('Norm')\n",
    "plt.title('L1/L2 Norms vs. # Estimators for Non-Parallelizable Regressors')\n",
    "plt.legend()\n",
    "plt.savefig('results/norm_slow.png', bbox_inches='tight', pad_inches=0.2)\n",
    "\n",
    "print(\"Done analyzing results!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing model aboost-20 as classifier...\n",
      "\tGot 2405/2780 unpopular and 1208/1560 popular songs correct!\n",
      "Analyzing model gboost-20 as classifier...\n",
      "\tGot 2580/2780 unpopular and 989/1560 popular songs correct!\n",
      "Analyzing model extrat-100 as classifier...\n",
      "\tGot 2515/2780 unpopular and 1109/1560 popular songs correct!\n",
      "Analyzing model bagger-100 as classifier...\n",
      "\tGot 2514/2780 unpopular and 1110/1560 popular songs correct!\n",
      "Analyzing model forest-100 as classifier...\n",
      "\tGot 2506/2780 unpopular and 1109/1560 popular songs correct!\n",
      "Analyzing model bayesian as classifier...\n",
      "\tGot 2550/2780 unpopular and 1032/1560 popular songs correct!\n",
      "Analyzing model elastic as classifier...\n",
      "\tGot 2558/2780 unpopular and 1017/1560 popular songs correct!\n",
      "Analyzing model lasso as classifier...\n",
      "\tGot 2559/2780 unpopular and 1019/1560 popular songs correct!\n",
      "Analyzing model linear as classifier...\n",
      "\tGot 2548/2780 unpopular and 1030/1560 popular songs correct!\n",
      "Analyzing model linSVR as classifier...\n",
      "\tGot 1187/2780 unpopular and 1467/1560 popular songs correct!\n",
      "Analyzing model mlp as classifier...\n",
      "\tGot 2573/2780 unpopular and 822/1560 popular songs correct!\n"
     ]
    }
   ],
   "source": [
    "final_regressors = set()\n",
    "for model in random_regressors:\n",
    "    final_regressors.add(model)\n",
    "for model in fast_regressors:\n",
    "    final_regressors.add(model + \"-{}\".format(fast_estimators[-1]))\n",
    "for model in slow_regressors:\n",
    "    final_regressors.add(model + \"-{}\".format(slow_estimators[-1]))\n",
    "final_regressors = list(final_regressors) \n",
    "final_regressors.sort(key=lambda model: model[-3:] if model[-1] == '0' else model[:3])\n",
    "\n",
    "threshold = 50\n",
    "unpopular = popular = 0\n",
    "for i in range(len(y_test)):\n",
    "    if y_test[i] >= threshold:\n",
    "        popular += 1\n",
    "    else:\n",
    "        unpopular += 1\n",
    "class_results = {}\n",
    "for model in final_regressors:\n",
    "    print(\"Analyzing model {} as classifier...\".format(model))\n",
    "    pred = regressors[model].predict(x_test)\n",
    "    u_correct = p_correct = 0\n",
    "    for i in range(len(y_test)):\n",
    "        if y_test[i] >= threshold and pred[i] >= threshold:\n",
    "            p_correct += 1\n",
    "        elif y_test[i] < threshold and pred[i] < threshold:\n",
    "            u_correct += 1\n",
    "    class_results[model] = (u_correct, p_correct)\n",
    "    print(\"\\tGot {}/{} unpopular and {}/{} popular songs correct!\".format(\n",
    "        u_correct, unpopular, p_correct, popular\n",
    "    ))\n",
    "\n",
    "plt.clf()\n",
    "plt.figure(figsize=(6,4))\n",
    "index = np.arange(len(final_regressors))\n",
    "bar_width = 0.25\n",
    "opacity = 0.8\n",
    "\n",
    "plt.bar(index, list(map(\n",
    "    lambda model: class_results[model][0] / unpopular, final_regressors\n",
    ")), bar_width, alpha=opacity, color='b', label='% Unpopular Correct')\n",
    "plt.bar(index + bar_width, list(map(\n",
    "    lambda model: class_results[model][1] / popular, final_regressors\n",
    ")), bar_width, alpha=opacity, color='g', label='% Popular Correct')\n",
    "plt.bar(index + bar_width * 2, list(map(\n",
    "    lambda model: (class_results[model][0] + class_results[model][1]) / (unpopular + popular), final_regressors\n",
    ")), bar_width, alpha=opacity, color='r', label='% Overall Correct')\n",
    "\n",
    "plt.xlabel('Model')\n",
    "plt.ylabel('% Correct')\n",
    "plt.gca().set_ylim([0, 1])\n",
    "plt.title('% Unpopular/Popular Labels Correct vs. Model')\n",
    "plt.xticks(index + bar_width, tuple(final_regressors), rotation=90)\n",
    "plt.legend(loc=4)\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.savefig('results/correct.png')\n",
    "\n",
    "with open(\"results/correct.csv\", \"w+\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Model', 'Unpopular % Correct', 'Popular % Correct', 'Overall % Correct'])\n",
    "    for model in final_regressors:\n",
    "        writer.writerow([\n",
    "            model,\n",
    "            \"{:.3f}\".format(class_results[model][0] / unpopular), \n",
    "            \"{:.3f}\".format(class_results[model][1] / popular),\n",
    "            \"{:.3f}\".format((class_results[model][0] + class_results[model][1]) / (unpopular + popular))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit time: 2.12\n"
     ]
    }
   ],
   "source": [
    "print(\"Logistic Regression: Classification\")\n",
    "lr = LogisticRegression(n_jobs=8, tol=10 ** -10)\n",
    "start_time = time.time()\n",
    "lr.fit(x_train, (y_train >= 80).astype(np.int))\n",
    "end_time = time.time()\n",
    "print(\"Fit time: {:.2f}\".format(end_time - start_time))\n",
    "\n",
    "pred = lr.predict(x_test)\n",
    "print(sum(pred))\n",
    "total = 0\n",
    "correct = 0\n",
    "for i in range(len(y_test)):\n",
    "    val = y_test[i]\n",
    "    if val < 80:\n",
    "        total += 1\n",
    "        if pred[i] == 0:\n",
    "            correct += 1\n",
    "print(total, correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['artist_popularity', 'release_date', 'album_length', 'loudness',\n",
       "       'instrumentalness', 'duration', 'num_segments', 'acousticness',\n",
       "       'start_of_fade_out', 'num_tatums', 'energy', 'num_beats',\n",
       "       'num_sections', 'track_number', 'num_bars', 'danceability',\n",
       "       'valence', 'explicit', 'speechiness', 'time_signature_confidence',\n",
       "       'end_of_fade_in', 'mode_confidence', 'tempo', 'time_signature',\n",
       "       'liveness', 'tempo_confidence', 'mode', 'key', 'key_confidence'],\n",
       "      dtype='<U25')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mi = mutual_info_regression(x_data, y_data)\n",
    "feature_ranking = np.argsort(mi)[::-1]\n",
    "features = hdrs[2:]\n",
    "features.pop(features.index('popularity'))\n",
    "np.array(features)[feature_ranking]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{|l|l|}\\hline\n",
      "Model & Training Time (s) \\\\\\hline\n",
      "mlp & 2.02 \\\\\\hline\n",
      "linSVR & 9.24 \\\\\\hline\n",
      "elastic & 1.67 \\\\\\hline\n",
      "lasso & 1.65 \\\\\\hline\n",
      "\\end{tabular}\n"
     ]
    }
   ],
   "source": [
    "def table(csv):\n",
    "    with open(csv) as f:\n",
    "        n = None\n",
    "        for line in f:\n",
    "            tokens = line.strip().split(',')\n",
    "            if n is None:\n",
    "                n = len(tokens)\n",
    "                print(\"\\\\begin{tabular}{\" + \"l\".join([\"|\" for _ in range(n + 1)]) + \"}\\\\hline\")\n",
    "            print(\" & \".join(tokens) + \" \\\\\\\\\\\\hline\")\n",
    "        print(\"\\\\end{tabular}\")\n",
    "    \n",
    "table(\"results_full/train_times_other.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "timings = '''\n",
    "Starting on regressor linear Fit time: 0.03\n",
    "Starting on regressor bayesian Fit time: 0.05\n",
    "Starting on regressor mlp Fit time: 2.02\n",
    "Starting on regressor linSVR Fit time: 9.24\n",
    "Starting on regressor elastic Fit time: 1.67\n",
    "Starting on regressor lasso Fit time: 1.65\n",
    "Starting on regressor forest-1 Fit time: 0.84\n",
    "Starting on regressor forest-2 Fit time: 0.93\n",
    "Starting on regressor forest-3 Fit time: 0.93\n",
    "Starting on regressor forest-4 Fit time: 0.93\n",
    "Starting on regressor forest-5 Fit time: 1.04\n",
    "Starting on regressor forest-10 Fit time: 2.16\n",
    "Starting on regressor forest-20 Fit time: 3.32\n",
    "Starting on regressor forest-30 Fit time: 4.74\n",
    "Starting on regressor forest-40 Fit time: 6.69\n",
    "Starting on regressor forest-50 Fit time: 8.04\n",
    "Starting on regressor forest-100 Fit time: 15.24\n",
    "Starting on regressor bagger-1 Fit time: 0.80\n",
    "Starting on regressor bagger-2 Fit time: 1.15\n",
    "Starting on regressor bagger-3 Fit time: 1.15\n",
    "Starting on regressor bagger-4 Fit time: 1.25\n",
    "Starting on regressor bagger-5 Fit time: 1.37\n",
    "Starting on regressor bagger-10 Fit time: 2.50\n",
    "Starting on regressor bagger-20 Fit time: 4.09\n",
    "Starting on regressor bagger-30 Fit time: 6.05\n",
    "Starting on regressor bagger-40 Fit time: 8.02\n",
    "Starting on regressor bagger-50 Fit time: 11.62\n",
    "Starting on regressor bagger-100 Fit time: 19.53\n",
    "Starting on regressor extrat-1 Fit time: 0.32\n",
    "Starting on regressor extrat-2 Fit time: 0.32\n",
    "Starting on regressor extrat-3 Fit time: 0.32\n",
    "Starting on regressor extrat-4 Fit time: 0.32\n",
    "Starting on regressor extrat-5 Fit time: 0.42\n",
    "Starting on regressor extrat-10 Fit time: 0.73\n",
    "Starting on regressor extrat-20 Fit time: 1.15\n",
    "Starting on regressor extrat-30 Fit time: 1.76\n",
    "Starting on regressor extrat-40 Fit time: 2.27\n",
    "Starting on regressor extrat-50 Fit time: 2.90\n",
    "Starting on regressor extrat-100 Fit time: 5.48\n",
    "Starting on regressor gboost-1  Fit time: 0.18\n",
    "Starting on regressor gboost-2  Fit time: 0.28\n",
    "Starting on regressor gboost-3  Fit time: 0.38\n",
    "Starting on regressor gboost-4  Fit time: 0.45\n",
    "Starting on regressor gboost-5  Fit time: 0.54\n",
    "Starting on regressor gboost-10 Fit time: 0.96\n",
    "Starting on regressor gboost-20 Fit time: 1.78\n",
    "Starting on regressor aboost-1  Fit time: 1.18\n",
    "Starting on regressor aboost-2  Fit time: 2.14\n",
    "Starting on regressor aboost-3  Fit time: 3.15\n",
    "Starting on regressor aboost-4  Fit time: 4.26\n",
    "Starting on regressor aboost-5  Fit time: 5.30\n",
    "Starting on regressor aboost-10 Fit time: 10.28\n",
    "Starting on regressor aboost-20 Fit time: 19.77\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "times = list(filter(len, timings.split('\\n')))\n",
    "times = list(map(lambda l: l.split(\" \")[3:], times))\n",
    "times = {\n",
    "    d[0]: float(d[-1]) for d in times\n",
    "}\n",
    "    \n",
    "with open(\"results/train_times.csv\", \"w+\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['Model', 'Training Time (s)'])\n",
    "    for regressor in regressors:\n",
    "        writer.writerow([regressor, times[regressor]])\n",
    "\n",
    "plt.clf()\n",
    "for key in fast_regressors:\n",
    "    plt.plot(fast_estimators, list(map(\n",
    "        lambda e: times[key + '-{}'.format(e)], fast_estimators\n",
    "    )), label=fast_regressors[key][0])\n",
    "plt.xlabel('# Estimators')\n",
    "plt.ylabel('Train Time (s)')\n",
    "plt.title('Train Time vs. # Estimators for Parallelizable Regressors')\n",
    "plt.legend()\n",
    "plt.savefig('results/train_time_fast.png')\n",
    "\n",
    "plt.clf()\n",
    "for key in slow_regressors:\n",
    "    plt.plot(slow_estimators, list(map(\n",
    "        lambda e: times[key + '-{}'.format(e)], slow_estimators\n",
    "    )), label=slow_regressors[key][0])\n",
    "plt.xlabel('# Estimators')\n",
    "plt.ylabel('Train Time (s)')\n",
    "plt.title('Train Time vs. # Estimators for Non-Parallelizable Regressors')\n",
    "plt.legend()\n",
    "plt.savefig('results/train_time_slow.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2780 1560\n"
     ]
    }
   ],
   "source": [
    "print(unpopular, popular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
